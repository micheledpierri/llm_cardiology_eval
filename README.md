[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![Requirements](https://img.shields.io/badge/dependencies-requirements.txt-green.svg)](./requirements.txt)


# Evaluating Large Language Models in Cardiology

This repository contains all data, code, and figures related to the study:

**"Evaluating Large Language Models in Cardiology: A Comparative Study of ChatGPT, Claude, and Gemini"**

## ğŸ“„ Abstract

This study systematically compares the performance of three large language models (ChatGPT, Claude, and Gemini) in cardiology-related clinical scenarios. Using 70 simulated prompts representing both pre- and post-diagnostic phases and two user profiles (patient and doctor), responses were rated by three board-certified cardiologists on four quality criteria: scientific accuracy, completeness, clarity, and coherence. Statistical analyses confirmed ChatGPTâ€™s superiority, though no model achieved maximal performance. Results support the need for domain-specific fine-tuning and human-in-the-loop oversight.

## ğŸ§ª Study Design

- Models Evaluated: ChatGPT (OpenAI), Claude (Anthropic), Gemini (Google DeepMind)
- Prompts: 70 clinical questions stratified by diagnostic phase and user type
- Scoring: 5-point Likert scale, four criteria
- Reviewers: 3 blinded cardiologists
- Evaluation Period: Septemberâ€“December 2024

## ğŸ“Š Methods

- Non-parametric tests (Kruskalâ€“Wallis, Dunnâ€™s test, Mannâ€“Whitney U)
- Inter-rater reliability (Kendallâ€™s W, Weighted Kappa)
- Sensitivity analysis (leave-one-reviewer-out)
- All scripts written in Python 

## ğŸ“ Repository Structure

```text
llm-cardiology-eval/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ 1_Pilot.xlsx
â”‚   â””â”€â”€ 2_Data.xlslx
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 1_Power_analysis.py
â”‚   â”œâ”€â”€ 2_reliability_analysis.py
â”‚   â”œâ”€â”€ 3_Statistical_Analysis.py
â”‚   â”œâ”€â”€ 4_Sensitivity_analysis.py
â”‚   â”œâ”€â”€ 5_Figure_1.py
â”‚   â”œâ”€â”€ 6_Figure_2.py
â”‚   â”œâ”€â”€ 7_Figure_3.py
â”‚   â””â”€â”€ 8_Figure_4.py
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ Figure1.pdf
â”‚   â”œâ”€â”€ Figure2.pdf
â”‚   â”œâ”€â”€ Figure_DiagnosticPhase_wErrorBars.pdf
â”‚   â””â”€â”€ Figure_UserType.pdf
â”œâ”€â”€ tables/
â”‚   â”œâ”€â”€ reliability_kappa.csv
â”‚   â”œâ”€â”€ reliability_kendall.csv
â”‚   â”œâ”€â”€ reliability_kendall_friedman.csv
â”‚   â”œâ”€â”€ sensitivity_analysis.csv
â”‚   â”œâ”€â”€ stat_analysis_descriptive.csv
â”‚   â”œâ”€â”€ stat_analysis_diagnostic_phase.csv
â”‚   â”œâ”€â”€ stat_analysis_kruskal.csv
â”‚   â”œâ”€â”€ stat_analysis_posthoc_dunn.csv
â”‚   â”œâ”€â”€ stat_analysis_power.csv
â”‚   â”œâ”€â”€ stat_analysis_shapiro.csv
â”‚   â””â”€â”€ stat_analysis_user_type.csv
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ—ƒï¸ Data Availability

All data used in this study are available in anonymized form under the `/data` directory. Original prompts, evaluation scores, and reliability statistics are included.

## â–¶ï¸ Reproducing the Analysis

1. Clone the repository:
   git clone https://github.com/micheledpierri/llm-cardiology-eval.git
   cd llm-cardiology-eval

2. Create virtual environment and install dependencies:
   python -m venv venv
   source venv/bin/activate  (on Windows: venv\Scripts\activate)
   pip install -r requirements.txt

3. Run main analysis:
   python scripts/analysis_main.py

âš ï¸ Scripts expect data files to be in the same folder. Copy `.csv`/`.xlsx` files into the script directory before running.


## ğŸ“œ License

This project is licensed under the MIT License â€“ see the LICENSE file for details.


## ğŸ¤ Contact

For questions, please contact: micheledanilo.pierri@ospedaliriuniti.marche.it
